{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de1b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6538575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
      "JAX default backend: tpu\n",
      "Number of TPU devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Configure JAX to use TPU\n",
    "os.environ['JAX_PLATFORMS'] = 'tpu'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import copy\n",
    "\n",
    "import flax.linen as nn\n",
    "from ml_collections import config_dict\n",
    "\n",
    "# Add the md4 module to the path\n",
    "sys.path.append('/home/wuhao/md4')\n",
    "\n",
    "from md4.configs.md4 import molecular\n",
    "from md4.models import utils as model_utils\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX default backend: {jax.default_backend()}\")\n",
    "print(f\"Number of TPU devices: {len(jax.devices())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b29c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model type: md4\n",
      "  Dataset: pubchem_large\n",
      "  Vocab size: 1023\n",
      "  Max length: 128\n",
      "  Feature dim: 64\n",
      "  Number of layers: 12\n",
      "  Number of heads: 12\n",
      "  Dropout rate: 0.02\n",
      "  Fingerprint dim: 2048\n",
      "  Timesteps: 1000\n",
      "  Batch size: 1024\n",
      "\n",
      "Model created: <class 'md4.models.diffusion.md4.MD4'>\n",
      "Model: MD4(\n",
      "    # attributes\n",
      "    data_shape = (128,)\n",
      "    cont_time = True\n",
      "    timesteps = 1000\n",
      "    feature_dim = 64\n",
      "    num_heads = 12\n",
      "    antithetic_time_sampling = True\n",
      "    n_layers = 12\n",
      "    n_dit_layers = 0\n",
      "    dit_num_heads = 12\n",
      "    dit_hidden_size = 768\n",
      "    ch_mult = (1,)\n",
      "    vocab_size = 1023\n",
      "    noise_schedule_type = 'cosine'\n",
      "    dropout_rate = 0.02\n",
      "    use_attn_dropout = True\n",
      "    mlp_type = 'swiglu'\n",
      "    depth_scaled_init = True\n",
      "    cond_type = 'adaln_zero'\n",
      "    outside_embed = True\n",
      "    time_features = 't'\n",
      "    classes = -1\n",
      "    sampler = 'ancestral'\n",
      "    sampling_grid = 'uniform'\n",
      "    topp = 0.98\n",
      "    model_sharding = False\n",
      "    fingerprint_dim = 2048\n",
      "    atom_type_size = 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the molecular configuration\n",
    "config = molecular.get_config()\n",
    "config.vocab_size = 1023\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model type: {config.model_type}\")\n",
    "print(f\"  Dataset: {config.dataset}\")\n",
    "print(f\"  Vocab size: {config.vocab_size}\")\n",
    "print(f\"  Max length: {config.max_length}\")\n",
    "print(f\"  Feature dim: {config.feature_dim}\")\n",
    "print(f\"  Number of layers: {config.n_layers}\")\n",
    "print(f\"  Number of heads: {config.num_heads}\")\n",
    "print(f\"  Dropout rate: {config.dropout_rate}\")\n",
    "print(f\"  Fingerprint dim: {config.fingerprint_dim}\")\n",
    "print(f\"  Timesteps: {config.timesteps}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "\n",
    "# Create the model\n",
    "model = model_utils.get_model(config)\n",
    "print(f\"\\nModel created: {type(model)}\")\n",
    "print(f\"Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a727d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (8, 128)\n",
      "Conditioning fingerprint shape: (8, 2048)\n",
      "\n",
      "Initializing model parameters...\n",
      "Model output keys: ['loss', 'loss_diff', 'loss_prior', 'loss_recon']\n",
      "Output loss: 8.797445297241211\n",
      "Number of parameters: 91,821,120\n",
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with dummy data\n",
    "batch_size = 8  # Use smaller batch size for performance testing\n",
    "seq_length = config.max_length\n",
    "\n",
    "# Create dummy inputs\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, sample_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "# Input shape for molecular data (SMILES tokens)\n",
    "dummy_input = jnp.ones((batch_size, seq_length), dtype=\"int32\")\n",
    "\n",
    "# Create conditioning (fingerprint)\n",
    "conditioning = {\n",
    "    \"fingerprint\": jnp.zeros((batch_size, config.fingerprint_dim), dtype=\"int32\"),\n",
    "}\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Conditioning fingerprint shape: {conditioning['fingerprint'].shape}\")\n",
    "\n",
    "# Initialize the model\n",
    "print(\"\\nInitializing model parameters...\")\n",
    "output, variables = model.init_with_output(\n",
    "    {\"sample\": sample_rng, \"params\": init_rng},\n",
    "    dummy_input,\n",
    "    cond=conditioning,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "params = variables[\"params\"]\n",
    "state = {k: v for k, v in variables.items() if k != \"params\"}\n",
    "\n",
    "print(f\"Model output keys: {list(output.keys())}\")\n",
    "print(f\"Output loss: {output.get('loss', 'N/A')}\")\n",
    "print(f\"Number of parameters: {sum(x.size for x in jax.tree_util.tree_leaves(params)):,}\")\n",
    "print(\"Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import parameter_overview\n",
    "print(\"\\nParameter overview:\")\n",
    "overview = parameter_overview.get_parameter_overview(params)\n",
    "print(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efda242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE TESTING - ACTUAL TRAIN STEP\n",
      "============================================================\n",
      "Created TrainState with step: 0\n",
      "Optimizer state initialized: True\n",
      "Metrics class keys: ['learning_rate', 'loss', 'loss_diff', 'loss_prior', 'loss_recon']\n",
      "\n",
      "Testing actual train step performance:\n",
      "Sequence length: 128\n",
      "Number of runs per batch size: 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Testing batch size 32...\n",
      "  Warming up for batch size 32...\n"
     ]
    }
   ],
   "source": [
    "# Performance testing - Train step timing using actual train.py functions\n",
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE TESTING - ACTUAL TRAIN STEP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "from md4.train import TrainState, get_learning_rate, create_metrics_class_from_keys, loss_fn\n",
    "\n",
    "# Create learning rate schedule function like in actual training\n",
    "num_train_steps = 10000  # Dummy value for schedule\n",
    "schedule_fn = functools.partial(\n",
    "    get_learning_rate,\n",
    "    base_learning_rate=config.learning_rate,\n",
    "    num_steps=num_train_steps,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    schedule_type=getattr(config, 'learning_rate_schedule', 'cosine'),\n",
    ")\n",
    "\n",
    "# Create optimizer exactly like in actual training\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(config.clip) if config.clip > 0.0 else optax.identity(),\n",
    "    optax.adamw(\n",
    "        schedule_fn,\n",
    "        b1=0.9,\n",
    "        b2=config.b2,\n",
    "        weight_decay=config.weight_decay,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create TrainState exactly like in actual training\n",
    "train_state = TrainState(\n",
    "    step=0,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    params=params,\n",
    "    ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "    opt_state=optimizer.init(params),\n",
    "    state=state,\n",
    ")\n",
    "\n",
    "# Create metrics class like in actual training\n",
    "# Get output keys from a dummy forward pass\n",
    "dummy_output, _ = model.init_with_output(\n",
    "    {\"sample\": jax.random.PRNGKey(0), \"params\": jax.random.PRNGKey(1)},\n",
    "    dummy_input,\n",
    "    cond=conditioning,\n",
    "    train=False,\n",
    ")\n",
    "metric_keys = sorted(list(dummy_output.keys()) + [\"learning_rate\"])\n",
    "train_metrics_class = create_metrics_class_from_keys(metric_keys)\n",
    "\n",
    "print(f\"Created TrainState with step: {train_state.step}\")\n",
    "print(f\"Optimizer state initialized: {train_state.opt_state is not None}\")\n",
    "print(f\"Metrics class keys: {metric_keys}\")\n",
    "\n",
    "# Create a simplified train step function without pmap operations\n",
    "@jax.jit\n",
    "def train_step_fn(train_state, batch):\n",
    "    \"\"\"Simplified train step function without pmap for performance testing.\"\"\"\n",
    "    rng, new_rng = jax.random.split(train_state.rng)\n",
    "    # Remove the pmap-specific fold_in operation\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, (new_state, metrics_dict)), grads = grad_fn(\n",
    "        train_state.params, train_state.state, rng, model, batch, train=True\n",
    "    )\n",
    "    \n",
    "    # Apply optimizer updates (without pmean for gradients)\n",
    "    updates, new_opt_state = optimizer.update(\n",
    "        grads, train_state.opt_state, train_state.params\n",
    "    )\n",
    "    new_params = optax.apply_updates(train_state.params, updates)\n",
    "    \n",
    "    # Handle EMA if configured\n",
    "    ema_rate = getattr(config, 'ema_rate', 0.0)\n",
    "    if ema_rate > 0.0:\n",
    "        new_ema_params = jax.tree_util.tree_map(\n",
    "            lambda x, y: x + (1.0 - ema_rate) * (y - x),\n",
    "            train_state.ema_params,\n",
    "            new_params,\n",
    "        )\n",
    "    else:\n",
    "        new_ema_params = None\n",
    "        \n",
    "    new_train_state = train_state.replace(\n",
    "        step=train_state.step + 1,\n",
    "        rng=new_rng,\n",
    "        params=new_params,\n",
    "        ema_params=new_ema_params,\n",
    "        opt_state=new_opt_state,\n",
    "        state=new_state,\n",
    "    )\n",
    "\n",
    "    \n",
    "    return new_train_state\n",
    "\n",
    "# Performance testing with different batch sizes\n",
    "batch_sizes = [64, 128]  # Start with larger batch sizes since this is more realistic\n",
    "num_runs = 10\n",
    "\n",
    "print(f\"\\nTesting actual train step performance:\")\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Number of runs per batch size: {num_runs}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\nTesting batch size {bs}...\")\n",
    "    \n",
    "    # Create batch in the format expected by train.py\n",
    "    batch = {\n",
    "        \"smiles\": jnp.ones((bs, seq_length), dtype=\"int32\"),\n",
    "        \"fingerprint\": jnp.zeros((bs, config.fingerprint_dim), dtype=\"int32\"),\n",
    "    }\n",
    "    \n",
    "    # Reset train state for each batch size\n",
    "    test_train_state = TrainState(\n",
    "        step=0,\n",
    "        rng=jax.random.PRNGKey(42),\n",
    "        params=params,\n",
    "        ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "        opt_state=optimizer.init(params),\n",
    "        state=state,\n",
    "    )\n",
    "    \n",
    "    # Warm up for this batch size (important for JIT compilation)\n",
    "    print(f\"  Warming up for batch size {bs}...\")\n",
    "    for _ in range(3):\n",
    "        test_train_state = train_step_fn(\n",
    "            train_state=test_train_state, \n",
    "            batch=batch\n",
    "        )\n",
    "    \n",
    "    # Reset for timing\n",
    "    test_train_state = TrainState(\n",
    "        step=0,\n",
    "        rng=jax.random.PRNGKey(42),\n",
    "        params=params,\n",
    "        ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "        opt_state=optimizer.init(params),\n",
    "        state=state,\n",
    "    )\n",
    "    \n",
    "    # Time multiple runs\n",
    "    print(f\"  Running {num_runs} timed iterations...\")\n",
    "    times = []\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        new_train_state = train_step_fn(\n",
    "            train_state=test_train_state,\n",
    "            batch=batch\n",
    "        )\n",
    "        # Block until computation is complete\n",
    "        jax.block_until_ready([new_train_state])\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "        # Use updated state for next iteration (realistic training simulation)\n",
    "        test_train_state = new_train_state\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    min_time = min(times)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    # Calculate throughput\n",
    "    samples_per_sec = bs / avg_time\n",
    "    tokens_per_sec = bs * seq_length / avg_time\n",
    "    \n",
    "    print(f\"Batch size {bs:3d}: {avg_time*1000:6.2f}ms avg ({min_time*1000:5.2f}-{max_time*1000:5.2f}ms) | \"\n",
    "          f\"{samples_per_sec:6.1f} samples/sec | {tokens_per_sec:8.0f} tokens/sec\")\n",
    "    \n",
    "    results.append({\n",
    "        'batch_size': bs,\n",
    "        'avg_time': avg_time,\n",
    "        'min_time': min_time,\n",
    "        'max_time': max_time,\n",
    "        'samples_per_sec': samples_per_sec,\n",
    "        'tokens_per_sec': tokens_per_sec\n",
    "    })\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b256ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Model: md4 with 12 layers, 12 heads\n",
      "Sequence length: 128, Vocab size: 1024\n",
      "Feature dimension: 64, Fingerprint dimension: 2048\n",
      "Total parameters: 91,821,952\n",
      "Device: TPU_0(process=0,(0,0,0,0))\n",
      "\n",
      "Optimal batch size: 64 (1305923 tokens/sec)\n",
      "\n",
      "Scaling analysis:\n",
      "  1 -> 4: 0.90x efficiency (3.60x throughput for 4.0x batch size)\n",
      "  4 -> 8: 0.88x efficiency (1.77x throughput for 2.0x batch size)\n",
      "  8 -> 16: 0.85x efficiency (1.70x throughput for 2.0x batch size)\n",
      "  16 -> 32: 0.69x efficiency (1.38x throughput for 2.0x batch size)\n",
      "  32 -> 64: 0.51x efficiency (1.02x throughput for 2.0x batch size)\n",
      "  64 -> 128: 0.36x efficiency (0.72x throughput for 2.0x batch size)\n",
      "  128 -> 256: 0.43x efficiency (0.85x throughput for 2.0x batch size)\n",
      "\n",
      "Recommendations:\n",
      "- Consider using smaller batch sizes for better efficiency\n",
      "- For single inference: ~1.5ms per sample\n",
      "- For batch processing: use batch size 64 for 1305923 tokens/sec\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze and summarize results\n",
    "print(\"\\nPERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if results:\n",
    "    print(f\"Model: {config.model_type} with {config.n_layers} layers, {config.num_heads} heads\")\n",
    "    print(f\"Sequence length: {seq_length}, Vocab size: {config.vocab_size}\")\n",
    "    print(f\"Feature dimension: {config.feature_dim}, Fingerprint dimension: {config.fingerprint_dim}\")\n",
    "    print(f\"Total parameters: {sum(x.size for x in jax.tree_util.tree_leaves(params)):,}\")\n",
    "    print(f\"Device: {jax.devices()[0]}\")\n",
    "    print()\n",
    "    \n",
    "    # Find optimal batch size (highest throughput)\n",
    "    best_throughput = max(results, key=lambda x: x['tokens_per_sec'])\n",
    "    print(f\"Optimal batch size: {best_throughput['batch_size']} \"\n",
    "          f\"({best_throughput['tokens_per_sec']:.0f} tokens/sec)\")\n",
    "    \n",
    "    # Memory efficiency analysis\n",
    "    print(\"\\nScaling analysis:\")\n",
    "    for i in range(1, len(results)):\n",
    "        prev_bs = results[i-1]['batch_size']\n",
    "        curr_bs = results[i]['batch_size']\n",
    "        prev_tps = results[i-1]['tokens_per_sec']\n",
    "        curr_tps = results[i]['tokens_per_sec']\n",
    "        \n",
    "        bs_ratio = curr_bs / prev_bs\n",
    "        tps_ratio = curr_tps / prev_tps\n",
    "        efficiency = tps_ratio / bs_ratio\n",
    "        \n",
    "        print(f\"  {prev_bs} -> {curr_bs}: {efficiency:.2f}x efficiency \"\n",
    "              f\"({tps_ratio:.2f}x throughput for {bs_ratio:.1f}x batch size)\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    if best_throughput['batch_size'] < max(r['batch_size'] for r in results):\n",
    "        print(\"- Consider using smaller batch sizes for better efficiency\")\n",
    "    else:\n",
    "        print(\"- Larger batch sizes provide better throughput\")\n",
    "    \n",
    "    print(f\"- For single inference: ~{results[0]['avg_time']*1000:.1f}ms per sample\")\n",
    "    print(f\"- For batch processing: use batch size {best_throughput['batch_size']} \"\n",
    "          f\"for {best_throughput['tokens_per_sec']:.0f} tokens/sec\")\n",
    "else:\n",
    "    print(\"No performance results available.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5dde11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
