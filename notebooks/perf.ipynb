{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5de1b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6538575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
      "JAX default backend: tpu\n",
      "Number of TPU devices: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Configure JAX to use TPU\n",
    "os.environ['JAX_PLATFORMS'] = 'tpu'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import copy\n",
    "import copy\n",
    "\n",
    "import flax.linen as nn\n",
    "from ml_collections import config_dict\n",
    "\n",
    "# Add the md4 module to the path\n",
    "sys.path.append('/home/wuhao/md4')\n",
    "\n",
    "from md4.configs.md4 import molecular\n",
    "from md4.models import utils as model_utils\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX default backend: {jax.default_backend()}\")\n",
    "print(f\"Number of TPU devices: {len(jax.devices())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2b29c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model type: md4\n",
      "  Dataset: pubchem_large\n",
      "  Vocab size: 1023\n",
      "  Max length: 128\n",
      "  Feature dim: 64\n",
      "  Number of layers: 12\n",
      "  Number of heads: 12\n",
      "  Dropout rate: 0.02\n",
      "  Fingerprint dim: 2048\n",
      "  Timesteps: 1000\n",
      "  Batch size: 1024\n",
      "\n",
      "Model created: <class 'md4.models.diffusion.md4.MD4'>\n",
      "Model: MD4(\n",
      "    # attributes\n",
      "    data_shape = (128,)\n",
      "    cont_time = True\n",
      "    timesteps = 1000\n",
      "    feature_dim = 64\n",
      "    num_heads = 12\n",
      "    antithetic_time_sampling = True\n",
      "    n_layers = 12\n",
      "    n_dit_layers = 0\n",
      "    dit_num_heads = 12\n",
      "    dit_hidden_size = 768\n",
      "    ch_mult = (1,)\n",
      "    vocab_size = 1023\n",
      "    noise_schedule_type = 'cosine'\n",
      "    dropout_rate = 0.02\n",
      "    use_attn_dropout = True\n",
      "    mlp_type = 'swiglu'\n",
      "    depth_scaled_init = True\n",
      "    cond_type = 'adaln_zero'\n",
      "    outside_embed = True\n",
      "    time_features = 't'\n",
      "    classes = -1\n",
      "    sampler = 'ancestral'\n",
      "    sampling_grid = 'uniform'\n",
      "    topp = 0.98\n",
      "    model_sharding = False\n",
      "    fingerprint_dim = 2048\n",
      "    atom_type_size = 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the molecular configuration\n",
    "config = molecular.get_config()\n",
    "config.vocab_size = 1023\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model type: {config.model_type}\")\n",
    "print(f\"  Dataset: {config.dataset}\")\n",
    "print(f\"  Vocab size: {config.vocab_size}\")\n",
    "print(f\"  Max length: {config.max_length}\")\n",
    "print(f\"  Feature dim: {config.feature_dim}\")\n",
    "print(f\"  Number of layers: {config.n_layers}\")\n",
    "print(f\"  Number of heads: {config.num_heads}\")\n",
    "print(f\"  Dropout rate: {config.dropout_rate}\")\n",
    "print(f\"  Fingerprint dim: {config.fingerprint_dim}\")\n",
    "print(f\"  Timesteps: {config.timesteps}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "\n",
    "# Create the model\n",
    "model = model_utils.get_model(config)\n",
    "print(f\"\\nModel created: {type(model)}\")\n",
    "print(f\"Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a727d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (8, 128)\n",
      "Conditioning fingerprint shape: (8, 2048)\n",
      "\n",
      "Initializing model parameters...\n",
      "Model output keys: ['loss', 'loss_diff', 'loss_prior', 'loss_recon']\n",
      "Output loss: 8.797445297241211\n",
      "Number of parameters: 91,821,120\n",
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with dummy data\n",
    "batch_size = 8  # Use smaller batch size for performance testing\n",
    "seq_length = config.max_length\n",
    "\n",
    "# Create dummy inputs\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rng, sample_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "# Input shape for molecular data (SMILES tokens)\n",
    "dummy_input = jnp.ones((batch_size, seq_length), dtype=\"int32\")\n",
    "\n",
    "# Create conditioning (fingerprint)\n",
    "conditioning = {\n",
    "    \"fingerprint\": jnp.zeros((batch_size, config.fingerprint_dim), dtype=\"int32\"),\n",
    "}\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Conditioning fingerprint shape: {conditioning['fingerprint'].shape}\")\n",
    "\n",
    "# Initialize the model\n",
    "print(\"\\nInitializing model parameters...\")\n",
    "output, variables = model.init_with_output(\n",
    "    {\"sample\": sample_rng, \"params\": init_rng},\n",
    "    dummy_input,\n",
    "    cond=conditioning,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "params = variables[\"params\"]\n",
    "state = {k: v for k, v in variables.items() if k != \"params\"}\n",
    "\n",
    "print(f\"Model output keys: {list(output.keys())}\")\n",
    "print(f\"Output loss: {output.get('loss', 'N/A')}\")\n",
    "print(f\"Number of parameters: {sum(x.size for x in jax.tree_util.tree_leaves(params)):,}\")\n",
    "print(\"Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clu import parameter_overview\n",
    "print(\"\\nParameter overview:\")\n",
    "overview = parameter_overview.get_parameter_overview(params)\n",
    "print(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2efda242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERFORMANCE TESTING - ACTUAL TRAIN STEP WITH MICROBATCHES\n",
      "============================================================\n",
      "Created TrainState with step: 0\n",
      "Optimizer state initialized: True\n",
      "Metrics class keys: ['learning_rate', 'loss', 'loss_diff', 'loss_prior', 'loss_recon']\n",
      "\n",
      "Testing actual train step performance with microbatches:\n",
      "Sequence length: 128\n",
      "Number of runs per configuration: 10\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Testing 64 (no microbatch)...\n",
      "  Warming up for 64 (no microbatch)...\n",
      "Created TrainState with step: 0\n",
      "Optimizer state initialized: True\n",
      "Metrics class keys: ['learning_rate', 'loss', 'loss_diff', 'loss_prior', 'loss_recon']\n",
      "\n",
      "Testing actual train step performance with microbatches:\n",
      "Sequence length: 128\n",
      "Number of runs per configuration: 10\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Testing 64 (no microbatch)...\n",
      "  Warming up for 64 (no microbatch)...\n",
      "  Running 10 timed iterations...\n",
      "  Running 10 timed iterations...\n",
      "64 (no microbatch)  :  39.62ms avg (39.45-39.70ms) | 1615.4 samples/sec |   206766 tokens/sec\n",
      "\n",
      "Testing 64 (2 microbatches)...\n",
      "  Warming up for 64 (2 microbatches)...\n"
     ]
    },
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function train_step_fn at /tmp/ipykernel_68087/474776472.py:57 for jit. This concrete value was not available in Python because it depends on the value of the argument num_microbatches.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTracerBoolConversionError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 213\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Warming up for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     test_train_state = \u001b[43mtrain_step_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_train_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_microbatches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_microbatches\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Reset for timing\u001b[39;00m\n\u001b[32m    220\u001b[39m test_train_state = TrainState(\n\u001b[32m    221\u001b[39m     step=\u001b[32m0\u001b[39m,\n\u001b[32m    222\u001b[39m     rng=jax.random.PRNGKey(\u001b[32m42\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    226\u001b[39m     state=state,\n\u001b[32m    227\u001b[39m )\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_step_fn\u001b[39m\u001b[34m(train_state, batch, num_microbatches)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Remove the pmap-specific fold_in operation\u001b[39;00m\n\u001b[32m     63\u001b[39m grad_fn = jax.value_and_grad(loss_fn, has_aux=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_microbatches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mnum_microbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# Single batch processing\u001b[39;00m\n\u001b[32m     67\u001b[39m     (_, (new_state, metrics_dict)), grads = grad_fn(\n\u001b[32m     68\u001b[39m         train_state.params, train_state.state, rng, model, batch, train=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Microbatch processing (copied from train.py)\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/md4/.venv/lib/python3.13/site-packages/jax/_src/core.py:1661\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1661\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[31mTracerBoolConversionError\u001b[39m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function train_step_fn at /tmp/ipykernel_68087/474776472.py:57 for jit. This concrete value was not available in Python because it depends on the value of the argument num_microbatches.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "# Performance testing - Train step timing using actual train.py functions\n",
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE TESTING - ACTUAL TRAIN STEP WITH MICROBATCHES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "from md4.train import TrainState, get_learning_rate, create_metrics_class_from_keys, loss_fn, merge_metrics\n",
    "\n",
    "# Create learning rate schedule function like in actual training\n",
    "num_train_steps = 10000  # Dummy value for schedule\n",
    "schedule_fn = functools.partial(\n",
    "    get_learning_rate,\n",
    "    base_learning_rate=config.learning_rate,\n",
    "    num_steps=num_train_steps,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    schedule_type=getattr(config, 'learning_rate_schedule', 'cosine'),\n",
    ")\n",
    "\n",
    "# Create optimizer exactly like in actual training\n",
    "optimizer = optax.chain(\n",
    "    optax.clip(config.clip) if config.clip > 0.0 else optax.identity(),\n",
    "    optax.adamw(\n",
    "        schedule_fn,\n",
    "        b1=0.9,\n",
    "        b2=config.b2,\n",
    "        weight_decay=config.weight_decay,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create TrainState exactly like in actual training\n",
    "train_state = TrainState(\n",
    "    step=0,\n",
    "    rng=jax.random.PRNGKey(42),\n",
    "    params=params,\n",
    "    ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "    opt_state=optimizer.init(params),\n",
    "    state=state,\n",
    ")\n",
    "\n",
    "# Create metrics class like in actual training\n",
    "# Get output keys from a dummy forward pass\n",
    "dummy_output, _ = model.init_with_output(\n",
    "    {\"sample\": jax.random.PRNGKey(0), \"params\": jax.random.PRNGKey(1)},\n",
    "    dummy_input,\n",
    "    cond=conditioning,\n",
    "    train=False,\n",
    ")\n",
    "metric_keys = sorted(list(dummy_output.keys()) + [\"learning_rate\"])\n",
    "train_metrics_class = create_metrics_class_from_keys(metric_keys)\n",
    "\n",
    "print(f\"Created TrainState with step: {train_state.step}\")\n",
    "print(f\"Optimizer state initialized: {train_state.opt_state is not None}\")\n",
    "print(f\"Metrics class keys: {metric_keys}\")\n",
    "\n",
    "# Create a train step function with microbatch support (like actual train.py)\n",
    "@jax.jit\n",
    "def train_step_fn(train_state, batch, num_microbatches=None):\n",
    "    \"\"\"Train step function with microbatch support for performance testing.\"\"\"\n",
    "    rng, new_rng = jax.random.split(train_state.rng)\n",
    "    # Remove the pmap-specific fold_in operation\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    \n",
    "    if num_microbatches is None or num_microbatches <= 1:\n",
    "        # Single batch processing\n",
    "        (_, (new_state, metrics_dict)), grads = grad_fn(\n",
    "            train_state.params, train_state.state, rng, model, batch, train=True\n",
    "        )\n",
    "    else:\n",
    "        # Microbatch processing (copied from train.py)\n",
    "        batch_size = next(iter(batch.values())).shape[0]\n",
    "        assert batch_size % num_microbatches == 0, (\n",
    "            \"Batch size isn't divided evenly by num_microbatches.\"\n",
    "        )\n",
    "        microbatch_size = batch_size // num_microbatches\n",
    "\n",
    "        def get_microbatch(batch, idx):\n",
    "            \"\"\"Fetch microbatch slice from possibly-packed input data.\"\"\"\n",
    "            offset = idx * microbatch_size\n",
    "            length = microbatch_size\n",
    "            starts = {k: [offset] + [0] * (b.ndim - 1) for k, b in batch.items()}\n",
    "            limits = {k: [length] + list(b.shape[1:]) for k, b in batch.items()}\n",
    "            return {\n",
    "                k: jax.lax.dynamic_slice(b, starts[k], limits[k])\n",
    "                for k, b in batch.items()\n",
    "            }\n",
    "\n",
    "        def metrics_and_grad(loop_cnt, rng, train_state_state):\n",
    "            _, mbrng = jax.random.split(rng)\n",
    "            mb = get_microbatch(batch, loop_cnt)\n",
    "\n",
    "            (_, (new_state, metrics_dict)), grads = grad_fn(\n",
    "                train_state.params, train_state_state, mbrng, model, mb, train=True\n",
    "            )\n",
    "            return metrics_dict, grads, new_state\n",
    "\n",
    "        def per_microbatch_train_step(loop_cnt, carry):\n",
    "            (rng, grad_accum, prev_metrics_dict, train_state_state) = carry\n",
    "            metrics_dict, grads, train_state_state = metrics_and_grad(\n",
    "                loop_cnt, rng, train_state_state\n",
    "            )\n",
    "\n",
    "            grad_accum = jax.tree.map(jnp.add, grad_accum, grads)\n",
    "            metrics_dict = jax.lax.cond(\n",
    "                loop_cnt == 0,\n",
    "                lambda _: metrics_dict,\n",
    "                lambda _: merge_metrics(prev_metrics_dict, metrics_dict),\n",
    "                None,\n",
    "            )\n",
    "            return rng, grad_accum, metrics_dict, train_state_state\n",
    "\n",
    "        # Initialize gradient accumulation loop state.\n",
    "        accum_dtype = jnp.float32\n",
    "        grad_accum_init = jax.tree.map(\n",
    "            lambda x: jnp.zeros(x.shape, accum_dtype), train_state.params\n",
    "        )\n",
    "        initial_metrics_shape, _, _ = jax.eval_shape(\n",
    "            metrics_and_grad,\n",
    "            loop_cnt=0,\n",
    "            rng=rng,\n",
    "            train_state_state=train_state.state,\n",
    "        )\n",
    "\n",
    "        initial_metrics = {\n",
    "            k: jnp.zeros(shape=v.shape, dtype=v.dtype)\n",
    "            for k, v in initial_metrics_shape.items()\n",
    "        }\n",
    "\n",
    "        loop_init = (\n",
    "            rng,\n",
    "            grad_accum_init,\n",
    "            initial_metrics,\n",
    "            train_state.state,\n",
    "        )\n",
    "        _, grads, metrics_dict, train_state_state = jax.lax.fori_loop(\n",
    "            0, num_microbatches, per_microbatch_train_step, loop_init\n",
    "        )\n",
    "        metrics_dict = jax.tree.map(lambda x: x / num_microbatches, metrics_dict)\n",
    "        new_state = train_state_state\n",
    "    \n",
    "    # Apply optimizer updates (without pmean for gradients since we're not using pmap)\n",
    "    updates, new_opt_state = optimizer.update(\n",
    "        grads, train_state.opt_state, train_state.params\n",
    "    )\n",
    "    new_params = optax.apply_updates(train_state.params, updates)\n",
    "    \n",
    "    # Handle EMA if configured\n",
    "    ema_rate = getattr(config, 'ema_rate', 0.0)\n",
    "    if ema_rate > 0.0:\n",
    "        new_ema_params = jax.tree_util.tree_map(\n",
    "            lambda x, y: x + (1.0 - ema_rate) * (y - x),\n",
    "            train_state.ema_params,\n",
    "            new_params,\n",
    "        )\n",
    "    else:\n",
    "        new_ema_params = None\n",
    "        \n",
    "    new_train_state = train_state.replace(\n",
    "        step=train_state.step + 1,\n",
    "        rng=new_rng,\n",
    "        params=new_params,\n",
    "        ema_params=new_ema_params,\n",
    "        opt_state=new_opt_state,\n",
    "        state=new_state,\n",
    "    )\n",
    "\n",
    "    return new_train_state\n",
    "\n",
    "# Performance testing with different batch sizes and microbatch configurations\n",
    "test_configs = [\n",
    "    {\"batch_size\": 64, \"num_microbatches\": None, \"name\": \"64 (no microbatch)\"},\n",
    "    {\"batch_size\": 64, \"num_microbatches\": 2, \"name\": \"64 (2 microbatches)\"},\n",
    "    {\"batch_size\": 64, \"num_microbatches\": 4, \"name\": \"64 (4 microbatches)\"},\n",
    "    {\"batch_size\": 128, \"num_microbatches\": None, \"name\": \"128 (no microbatch)\"},\n",
    "    {\"batch_size\": 128, \"num_microbatches\": 4, \"name\": \"128 (4 microbatches)\"},\n",
    "]\n",
    "num_runs = 10\n",
    "\n",
    "print(f\"\\nTesting actual train step performance with microbatches:\")\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Number of runs per configuration: {num_runs}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results = []\n",
    "\n",
    "for config_test in test_configs:\n",
    "    bs = config_test[\"batch_size\"]\n",
    "    num_microbatches = config_test[\"num_microbatches\"]\n",
    "    config_name = config_test[\"name\"]\n",
    "    \n",
    "    print(f\"\\nTesting {config_name}...\")\n",
    "    \n",
    "    # Create batch in the format expected by train.py\n",
    "    batch = {\n",
    "        \"smiles\": jnp.ones((bs, seq_length), dtype=\"int32\"),\n",
    "        \"fingerprint\": jnp.zeros((bs, config.fingerprint_dim), dtype=\"int32\"),\n",
    "    }\n",
    "    \n",
    "    # Reset train state for each configuration\n",
    "    test_train_state = TrainState(\n",
    "        step=0,\n",
    "        rng=jax.random.PRNGKey(42),\n",
    "        params=params,\n",
    "        ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "        opt_state=optimizer.init(params),\n",
    "        state=state,\n",
    "    )\n",
    "    \n",
    "    # Warm up for this configuration (important for JIT compilation)\n",
    "    print(f\"  Warming up for {config_name}...\")\n",
    "    for _ in range(3):\n",
    "        test_train_state = train_step_fn(\n",
    "            train_state=test_train_state, \n",
    "            batch=batch,\n",
    "            num_microbatches=num_microbatches\n",
    "        )\n",
    "    \n",
    "    # Reset for timing\n",
    "    test_train_state = TrainState(\n",
    "        step=0,\n",
    "        rng=jax.random.PRNGKey(42),\n",
    "        params=params,\n",
    "        ema_params=copy.deepcopy(params) if getattr(config, 'ema_rate', 0.0) > 0.0 else None,\n",
    "        opt_state=optimizer.init(params),\n",
    "        state=state,\n",
    "    )\n",
    "    \n",
    "    # Time multiple runs\n",
    "    print(f\"  Running {num_runs} timed iterations...\")\n",
    "    times = []\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        new_train_state = train_step_fn(\n",
    "            train_state=test_train_state,\n",
    "            batch=batch,\n",
    "            num_microbatches=num_microbatches\n",
    "        )\n",
    "        # Block until computation is complete\n",
    "        jax.block_until_ready([new_train_state])\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        \n",
    "        # Use updated state for next iteration (realistic training simulation)\n",
    "        test_train_state = new_train_state\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    min_time = min(times)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    # Calculate throughput\n",
    "    samples_per_sec = bs / avg_time\n",
    "    tokens_per_sec = bs * seq_length / avg_time\n",
    "    \n",
    "    microbatch_info = f\" ({num_microbatches} μb)\" if num_microbatches else \" (no μb)\"\n",
    "    print(f\"{config_name:20s}: {avg_time*1000:6.2f}ms avg ({min_time*1000:5.2f}-{max_time*1000:5.2f}ms) | \"\n",
    "          f\"{samples_per_sec:6.1f} samples/sec | {tokens_per_sec:8.0f} tokens/sec\")\n",
    "    \n",
    "    results.append({\n",
    "        'config_name': config_name,\n",
    "        'batch_size': bs,\n",
    "        'num_microbatches': num_microbatches,\n",
    "        'avg_time': avg_time,\n",
    "        'min_time': min_time,\n",
    "        'max_time': max_time,\n",
    "        'samples_per_sec': samples_per_sec,\n",
    "        'tokens_per_sec': tokens_per_sec\n",
    "    })\n",
    "\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Model: md4 with 12 layers, 12 heads\n",
      "Sequence length: 128, Vocab size: 1024\n",
      "Feature dimension: 64, Fingerprint dimension: 2048\n",
      "Total parameters: 91,821,952\n",
      "Device: TPU_0(process=0,(0,0,0,0))\n",
      "\n",
      "Optimal batch size: 64 (1305923 tokens/sec)\n",
      "\n",
      "Scaling analysis:\n",
      "  1 -> 4: 0.90x efficiency (3.60x throughput for 4.0x batch size)\n",
      "  4 -> 8: 0.88x efficiency (1.77x throughput for 2.0x batch size)\n",
      "  8 -> 16: 0.85x efficiency (1.70x throughput for 2.0x batch size)\n",
      "  16 -> 32: 0.69x efficiency (1.38x throughput for 2.0x batch size)\n",
      "  32 -> 64: 0.51x efficiency (1.02x throughput for 2.0x batch size)\n",
      "  64 -> 128: 0.36x efficiency (0.72x throughput for 2.0x batch size)\n",
      "  128 -> 256: 0.43x efficiency (0.85x throughput for 2.0x batch size)\n",
      "\n",
      "Recommendations:\n",
      "- Consider using smaller batch sizes for better efficiency\n",
      "- For single inference: ~1.5ms per sample\n",
      "- For batch processing: use batch size 64 for 1305923 tokens/sec\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze and summarize results\n",
    "print(\"\\nPERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if results:\n",
    "    print(f\"Model: {config.model_type} with {config.n_layers} layers, {config.num_heads} heads\")\n",
    "    print(f\"Sequence length: {seq_length}, Vocab size: {config.vocab_size}\")\n",
    "    print(f\"Feature dimension: {config.feature_dim}, Fingerprint dimension: {config.fingerprint_dim}\")\n",
    "    print(f\"Total parameters: {sum(x.size for x in jax.tree_util.tree_leaves(params)):,}\")\n",
    "    print(f\"Device: {jax.devices()[0]}\")\n",
    "    print()\n",
    "    \n",
    "    # Find optimal batch size (highest throughput)\n",
    "    best_throughput = max(results, key=lambda x: x['tokens_per_sec'])\n",
    "    print(f\"Optimal batch size: {best_throughput['batch_size']} \"\n",
    "          f\"({best_throughput['tokens_per_sec']:.0f} tokens/sec)\")\n",
    "    \n",
    "    # Memory efficiency analysis\n",
    "    print(\"\\nScaling analysis:\")\n",
    "    for i in range(1, len(results)):\n",
    "        prev_bs = results[i-1]['batch_size']\n",
    "        curr_bs = results[i]['batch_size']\n",
    "        prev_tps = results[i-1]['tokens_per_sec']\n",
    "        curr_tps = results[i]['tokens_per_sec']\n",
    "        \n",
    "        bs_ratio = curr_bs / prev_bs\n",
    "        tps_ratio = curr_tps / prev_tps\n",
    "        efficiency = tps_ratio / bs_ratio\n",
    "        \n",
    "        print(f\"  {prev_bs} -> {curr_bs}: {efficiency:.2f}x efficiency \"\n",
    "              f\"({tps_ratio:.2f}x throughput for {bs_ratio:.1f}x batch size)\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    if best_throughput['batch_size'] < max(r['batch_size'] for r in results):\n",
    "        print(\"- Consider using smaller batch sizes for better efficiency\")\n",
    "    else:\n",
    "        print(\"- Larger batch sizes provide better throughput\")\n",
    "    \n",
    "    print(f\"- For single inference: ~{results[0]['avg_time']*1000:.1f}ms per sample\")\n",
    "    print(f\"- For batch processing: use batch size {best_throughput['batch_size']} \"\n",
    "          f\"for {best_throughput['tokens_per_sec']:.0f} tokens/sec\")\n",
    "else:\n",
    "    print(\"No performance results available.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5dde11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
