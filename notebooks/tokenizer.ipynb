{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ae75f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Split, PreTokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd178dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8cdc65ddb146e2b99f134acb0f562c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9b7da2fb5d46c6b8165e2b11b0b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0b415f5e514f2190ac2961587caf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/21 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea46d0905dc414a9a901342d23bbda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/118536656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f370c01ba5b0471baa94981562b089eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"jablonkagroup/pubchem-smiles-molecular-formula\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d0310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['CID', 'smiles', 'molecular_formula'],\n",
       "    num_rows: 118536656\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5131c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eab6e6cb1e4b3abc98d01804c252bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a802a71d76804ec5a96041fa413db8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fe3d29c8c3421aa1b86186382e49c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329fb4c320644d4c82d13727482468c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d381e4b501e4e859ced99471717dc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/672 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "smiles_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"DeepChem/SmilesTokenizer_PubChem_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8763b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smiles_pre_tokenizer():\n",
    "    \"\"\"Create a pre-tokenizer suitable for SMILES strings.\"\"\"\n",
    "    # SMILES use specific characters - we'll split on common atom/bond boundaries\n",
    "    # This helps the tokenizer learn meaningful chemical substructures\n",
    "    return Sequence([\n",
    "        Split(pattern=r'(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])', behavior=\"isolated\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f80502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "special_tokens = [\n",
    "    \"[PAD]\",    # Padding token\n",
    "    \"[UNK]\",    # Unknown token  \n",
    "    \"[CLS]\",    # Classification token (start of sequence)\n",
    "    \"[SEP]\",    # Separator token (end of sequence)\n",
    "]\n",
    "\n",
    "# Initialize tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.pre_tokenizer = create_smiles_pre_tokenizer()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    # vocab_size=1024,\n",
    "    special_tokens=special_tokens,\n",
    "    min_frequency=10000,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(ds[\"smiles\"], trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b0e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to /mnt/workspace/md4/data/pubchem_large_tokenizer\n",
      "Vocabulary size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Add post-processor to add special tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "    \n",
    "# Create transformers tokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    ")\n",
    "\n",
    "output_dir = \"/mnt/workspace/md4/data\"\n",
    "tokenizer_name = \"pubchem_large_tokenizer\"\n",
    "# Save tokenizer\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tokenizer_path = os.path.join(output_dir, tokenizer_name)\n",
    "fast_tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "print(f\"Vocabulary size: {fast_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfa482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'CCCC', 'CCN', 'O', 'P', '(C)', '(=O)', 'OC', '[SEP]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.convert_ids_to_tokens(fast_tokenizer.encode(ds[\"smiles\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807853c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
