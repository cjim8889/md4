{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ae75f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/md4/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace, Sequence, Split, PreTokenizer\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd178dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 82335768/82335768 [00:31<00:00, 2655407.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"jablonkagroup/pubchem-smiles-molecular-formula\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d0310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['smiles', 'molecular_formula'],\n",
       "    num_rows: 82335768\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5131c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../data/pubchem_large_tokenizer_2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f972ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = smiles_tokenizer.encode(text=ds[\"molecular_formula\"][0], text_pair=ds[\"smiles\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc84f188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] C14H19 NO2 [SEP] OC1 [C@H]2 CO C[C@H]1 CN(C c1ccccc1) C2 [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_tokenizer.convert_tokens_to_string(smiles_tokenizer.convert_ids_to_tokens(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8763b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smiles_pre_tokenizer():\n",
    "    \"\"\"Create a pre-tokenizer suitable for SMILES strings.\"\"\"\n",
    "    # SMILES use specific characters - we'll split on common atom/bond boundaries\n",
    "    # This helps the tokenizer learn meaningful chemical substructures\n",
    "    return Sequence([\n",
    "        Split(pattern=r'(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])', behavior=\"isolated\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f80502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "special_tokens = [\n",
    "    \"[PAD]\",    # Padding token\n",
    "    \"[UNK]\",    # Unknown token  \n",
    "    \"[CLS]\",    # Classification token (start of sequence)\n",
    "    \"[SEP]\",    # Separator token (end of sequence)\n",
    "]\n",
    "\n",
    "# Initialize tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.pre_tokenizer = create_smiles_pre_tokenizer()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    # vocab_size=1024,\n",
    "    special_tokens=special_tokens,\n",
    "    min_frequency=10000,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(ds[\"smiles\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b0e7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to /mnt/workspace/md4/data/pubchem_large_tokenizer\n",
      "Vocabulary size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Add post-processor to add special tokens\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "    \n",
    "# Create transformers tokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    ")\n",
    "\n",
    "output_dir = \"/mnt/workspace/md4/data\"\n",
    "tokenizer_name = \"pubchem_large_tokenizer\"\n",
    "# Save tokenizer\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "tokenizer_path = os.path.join(output_dir, tokenizer_name)\n",
    "fast_tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_path}\")\n",
    "print(f\"Vocabulary size: {fast_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfa482b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'CCCC', 'CCN', 'O', 'P', '(C)', '(=O)', 'OC', '[SEP]']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.convert_ids_to_tokens(fast_tokenizer.encode(ds[\"smiles\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807853c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
