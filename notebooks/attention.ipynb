{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e794d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "\n",
    "import flax.linen as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/wuhao/md4')\n",
    "\n",
    "\n",
    "from md4.networks.transformer import Attention, apply_rotary_emb, Dropout1d, precompute_freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d47662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop‑in replacement for Attention that is *mathematically identical*\n",
    "    but packs 4 heads → 1 MXU‑friendly 256‑wide head (v6e TPUs).\n",
    "\n",
    "    Handles the grouped‑/multi‑query case exactly like Llama:\n",
    "        n_rep   = n_heads // n_kv_heads\n",
    "        K,V are projected for n_kv_heads and repeated n_rep times.\n",
    "    \n",
    "    \"\"\"\n",
    "    # ------------------------------ public arguments --------------------------\n",
    "    dim:          int\n",
    "    n_heads:      int\n",
    "    n_kv_heads:   int | None = None      # None ≡ n_heads (full MHA)\n",
    "    dropout_rate: float = 0.0\n",
    "    causal:       bool  = False\n",
    "    qkv_bias:     bool  = False\n",
    "    group:        int   = 4              # 4×64 = 256\n",
    "\n",
    "    # ------------------------------ setup -------------------------------------\n",
    "    def setup(self):\n",
    "        self._n_kv = self.n_heads if self.n_kv_heads is None else self.n_kv_heads\n",
    "        assert self.n_heads % self._n_kv == 0, \"n_heads must be multiple of n_kv_heads\"\n",
    "\n",
    "        self.n_rep     = self.n_heads // self._n_kv      # replication factor\n",
    "        self.head_dim  = self.dim // self.n_heads        # 64  (must divide)\n",
    "        self.mega_dim  = self.group * self.head_dim      # 256\n",
    "\n",
    "        # Projections: note the shapes match the original implementation\n",
    "        self.wq = nn.Dense(self.n_heads * self.head_dim,  use_bias=self.qkv_bias)\n",
    "        self.wk = nn.Dense(self._n_kv   * self.head_dim,  use_bias=self.qkv_bias)\n",
    "        self.wv = nn.Dense(self._n_kv   * self.head_dim,  use_bias=self.qkv_bias)\n",
    "        self.wo = nn.Dense(self.dim, use_bias=False)\n",
    "\n",
    "        if self.dropout_rate:\n",
    "            self.attn_dropout  = nn.Dropout(self.dropout_rate)\n",
    "            self.resid_dropout = Dropout1d(self.dropout_rate)\n",
    "\n",
    "    # ------------------------------ helpers -----------------------------------\n",
    "    @staticmethod\n",
    "    def _pack(x, group=4):\n",
    "        \"\"\"(B,T,H,64) → (B,T,G,256), where H = G*group.\"\"\"\n",
    "        B, T, H, D = x.shape\n",
    "        G = H // group\n",
    "        x = x.reshape(B, T, G, group, D)                    # (B,T,G,4,64)\n",
    "\n",
    "        # gather indices  [[0..63],[64..127],[128..191],[192..255]]\n",
    "        idx = (jnp.arange(group)[:, None] * D) + jnp.arange(D)   # (4,64)\n",
    "        buf = jnp.zeros((B, T, G, group * D), dtype=x.dtype)\n",
    "        buf = buf.at[..., idx].set(x)                       # scatter into zeros\n",
    "        return buf                                          # (B,T,G,256)\n",
    "\n",
    "    @staticmethod\n",
    "    def _unpack(y, group=4):\n",
    "        \"\"\"(B,T,G,256) → (B,T,H,64), H = G*group.\"\"\"\n",
    "        B, T, G, _ = y.shape\n",
    "        D = 64\n",
    "        idx = (jnp.arange(group)[:, None] * D) + jnp.arange(D)\n",
    "        y = y[..., idx]                                    # gather back\n",
    "        return y.reshape(B, T, G * group, D)\n",
    "\n",
    "    # ------------------------------ call --------------------------------------\n",
    "    def __call__(self, x, freqs_cos, freqs_sin, *, train=False):\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        # 1) Linear projections -------------------------------------------------\n",
    "        q = self.wq(x).reshape(B, T, self.n_heads, self.head_dim)      # (B,T,H,64)\n",
    "        k = self.wk(x).reshape(B, T, self._n_kv,  self.head_dim)       # (B,T,H_kv,64)\n",
    "        v = self.wv(x).reshape(B, T, self._n_kv,  self.head_dim)\n",
    "\n",
    "        # 2) RoPE ---------------------------------------------------------------\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cos, freqs_sin)\n",
    "\n",
    "        # 3) Repeat K,V across heads if GQA/MQA ---------------------------------\n",
    "        if self.n_rep > 1:\n",
    "            k = repeat_kv(k, self.n_rep)   # (B,T,H,64)\n",
    "            v = repeat_kv(v, self.n_rep)\n",
    "\n",
    "        # 4) Pack four heads → one 256‑wide mega‑head ---------------------------\n",
    "        q = self._pack(q, self.group)          # (B,T,G,256)\n",
    "        k = self._pack(k, self.group)\n",
    "        v = self._pack(v, self.group)\n",
    "\n",
    "        # 5) MXU‑friendly dot‑product ------------------------------------------\n",
    "        q = q.transpose(0, 2, 1, 3)            # (B,G,T,256)\n",
    "        k = k.transpose(0, 2, 3, 1)            # (B,G,256,T)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(self.head_dim) # still √64\n",
    "        attn  = jnp.matmul(q, k) * scale       # (B,G,T,T)\n",
    "\n",
    "        if self.causal:\n",
    "            mask = jnp.triu(jnp.full((T, T), -jnp.inf, attn.dtype), k=1)\n",
    "            attn = attn + mask                 # broadcast over (B,G)\n",
    "\n",
    "        attn = nn.softmax(attn, axis=-1)\n",
    "        if self.dropout_rate:\n",
    "            attn = self.attn_dropout(attn, deterministic=not train)\n",
    "\n",
    "        # 6) context ------------------------------------------------------------\n",
    "        v = v.transpose(0, 2, 1, 3)            # (B,G,T,256)\n",
    "        ctx = jnp.matmul(attn, v)              # (B,G,T,256)\n",
    "\n",
    "        # 7) Un‑pack and merge heads -------------------------------------------\n",
    "        ctx = ctx.transpose(0, 2, 1, 3)        # (B,T,G,256)\n",
    "        ctx = self._unpack(ctx, self.group)    # (B,T,H,64)\n",
    "        ctx = ctx.reshape(B, T, self.dim)      # (B,T,768)\n",
    "\n",
    "        # 8) Output projection --------------------------------------------------\n",
    "        out = self.wo(ctx)\n",
    "        if self.dropout_rate:\n",
    "            out = self.resid_dropout(out, deterministic=not train)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d753aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Attention Equivalence ===\n",
      "\n",
      "xq: (4, 12, 16, 64), xk: (4, 12, 16, 64), xv: (4, 12, 16, 64)\n",
      "scores: (4, 12, 16, 16)\n",
      "Model configurations:\n",
      "  Batch size: 4\n",
      "  Sequence length: 16\n",
      "  Dimension: 768\n",
      "  Number of heads: 12\n",
      "  Head dimension: 64\n",
      "  Group size (fused): 4\n",
      "\n",
      "Test 1: Without RoPE effect (freqs_cos=1, freqs_sin=0)\n",
      "xq: (4, 12, 16, 64), xk: (4, 12, 16, 64), xv: (4, 12, 16, 64)\n",
      "scores: (4, 12, 16, 16)\n",
      "  Max absolute difference: 2.78e+00\n",
      "  Mean absolute difference: 3.74e-01\n",
      "  Relative error: 1.31e+00\n",
      "  Outputs are equivalent: False\n",
      "\n",
      "Test 2: With RoPE applied\n",
      "xq: (4, 12, 16, 64), xk: (4, 12, 16, 64), xv: (4, 12, 16, 64)\n",
      "scores: (4, 12, 16, 16)\n",
      "  Max absolute difference: 2.15e+00\n",
      "  Mean absolute difference: 3.66e-01\n",
      "  Relative error: 1.29e+00\n",
      "  Outputs are equivalent with RoPE: False\n",
      "\n",
      "\n",
      "Test 4: Internal shape verification\n",
      "  Pack/unpack round-trip error: 0.00e+00\n",
      "  Pack/unpack preserves data: True\n",
      "\n",
      "=== Summary ===\n",
      "✓ Models are mathematically equivalent (no RoPE): False\n",
      "✓ Models are mathematically equivalent (with RoPE): False\n",
      "✓ Pack/unpack functions preserve data: True\n"
     ]
    }
   ],
   "source": [
    "def test_attention_equivalence():\n",
    "    \"\"\"Test if Attention and FusedHeadAttention are mathematically equivalent.\"\"\"\n",
    "    print(\"=== Testing Attention Equivalence ===\\n\")\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size = 4\n",
    "    seq_len = 16\n",
    "    dim = 12 * 64  # 768\n",
    "    n_heads = 12\n",
    "    dropout_rate = 0.0  # No dropout for deterministic comparison\n",
    "    \n",
    "    # Create models\n",
    "    attention_ref = Attention(\n",
    "        dim=dim, \n",
    "        n_heads=n_heads, \n",
    "        n_kv_heads=n_heads,\n",
    "        dropout_rate=dropout_rate,\n",
    "        qkv_bias=False,\n",
    "        causal=False\n",
    "    )\n",
    "    \n",
    "    attention_fused = FusedHeadAttention(\n",
    "        dim=dim,\n",
    "        n_heads=n_heads,\n",
    "        dropout_rate=dropout_rate,\n",
    "        causal=False,\n",
    "        qkv_bias=False,\n",
    "        group=4\n",
    "    )\n",
    "    \n",
    "    # Create test data\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    \n",
    "    # Precompute RoPE frequencies\n",
    "    head_dim = dim // n_heads\n",
    "    freqs_cos, freqs_sin = precompute_freqs_cis(head_dim, seq_len)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    params_ref = attention_ref.init(key1, x, freqs_cos, freqs_sin, train=False)\n",
    "    params_fused = attention_fused.init(key2, x, freqs_cos, freqs_sin, train=False)\n",
    "    \n",
    "    # Copy weights from reference to fused to ensure they're identical\n",
    "    params_fused_corrected = {\n",
    "        'params': {\n",
    "            'wq': params_ref['params']['wq'],\n",
    "            'wk': params_ref['params']['wk'], \n",
    "            'wv': params_ref['params']['wv'],\n",
    "            'wo': params_ref['params']['wo']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Model configurations:\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Sequence length: {seq_len}\")\n",
    "    print(f\"  Dimension: {dim}\")\n",
    "    print(f\"  Number of heads: {n_heads}\")\n",
    "    print(f\"  Head dimension: {head_dim}\")\n",
    "    print(f\"  Group size (fused): {attention_fused.group}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 1: Without RoPE effect (identity rotation)\n",
    "    print(\"Test 1: Without RoPE effect (freqs_cos=1, freqs_sin=0)\")\n",
    "    freqs_cos_identity = jnp.ones_like(freqs_cos)\n",
    "    freqs_sin_identity = jnp.zeros_like(freqs_sin)\n",
    "    \n",
    "    # Forward pass\n",
    "    output_ref = attention_ref.apply(params_ref, x, freqs_cos_identity, freqs_sin_identity, train=False)\n",
    "    output_fused = attention_fused.apply(\n",
    "        params_fused_corrected, x, freqs_cos_identity, freqs_sin_identity, train=False\n",
    "    )\n",
    "    \n",
    "    # Compare outputs\n",
    "    max_diff = jnp.max(jnp.abs(output_ref - output_fused))\n",
    "    mean_diff = jnp.mean(jnp.abs(output_ref - output_fused))\n",
    "    relative_error = mean_diff / (jnp.mean(jnp.abs(output_ref)) + 1e-8)\n",
    "    \n",
    "    print(f\"  Max absolute difference: {max_diff:.2e}\")\n",
    "    print(f\"  Mean absolute difference: {mean_diff:.2e}\")\n",
    "    print(f\"  Relative error: {relative_error:.2e}\")\n",
    "    print(f\"  Outputs are equivalent: {max_diff < 1e-5}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 2: With actual RoPE\n",
    "    print(\"Test 2: With RoPE applied\")\n",
    "    output_ref_rope = attention_ref.apply(params_ref, x, freqs_cos, freqs_sin, train=False)\n",
    "    output_fused_rope = attention_fused.apply(\n",
    "        params_fused_corrected, x, freqs_cos, freqs_sin, train=False\n",
    "    )\n",
    "    \n",
    "    # Compare outputs with RoPE\n",
    "    max_diff_rope = jnp.max(jnp.abs(output_ref_rope - output_fused_rope))\n",
    "    mean_diff_rope = jnp.mean(jnp.abs(output_ref_rope - output_fused_rope))\n",
    "    relative_error_rope = mean_diff_rope / (jnp.mean(jnp.abs(output_ref_rope)) + 1e-8)\n",
    "    \n",
    "    print(f\"  Max absolute difference: {max_diff_rope:.2e}\")\n",
    "    print(f\"  Mean absolute difference: {mean_diff_rope:.2e}\")\n",
    "    print(f\"  Relative error: {relative_error_rope:.2e}\")\n",
    "    print(f\"  Outputs are equivalent with RoPE: {max_diff_rope < 1e-5}\")\n",
    "    print()\n",
    "    \n",
    "    # Test 4: Detailed shape verification\n",
    "    print(\"\\nTest 4: Internal shape verification\")\n",
    "    \n",
    "    # Manually test the packing/unpacking functions\n",
    "    test_tensor = jax.random.normal(key, (2, 8, 12, 64))  # (B, T, H, D)\n",
    "    packed = _pack_heads(test_tensor, group=4)\n",
    "    unpacked = _unpack_heads(packed, group=4)\n",
    "    \n",
    "    pack_unpack_diff = jnp.max(jnp.abs(test_tensor - unpacked))\n",
    "    print(f\"  Pack/unpack round-trip error: {pack_unpack_diff:.2e}\")\n",
    "    print(f\"  Pack/unpack preserves data: {pack_unpack_diff < 1e-10}\")\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"✓ Models are mathematically equivalent (no RoPE): {max_diff < 1e-5}\")\n",
    "    print(f\"✓ Models are mathematically equivalent (with RoPE): {max_diff_rope < 1e-5}\")\n",
    "    print(f\"✓ Pack/unpack functions preserve data: {pack_unpack_diff < 1e-10}\")\n",
    "\n",
    "# Run the equivalence test\n",
    "test_attention_equivalence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fecae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4 (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
