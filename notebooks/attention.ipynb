{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e794d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "\n",
    "import flax.linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fffd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout1d(nn.Module):\n",
    "\n",
    "  dropout_rate: float = 0.0\n",
    "\n",
    "  def __call__(self, x, deterministic=True):\n",
    "    if (self.dropout_rate > 0.0) and not deterministic:\n",
    "      drop = jax.random.bernoulli(\n",
    "          self.make_rng('dropout'),\n",
    "          1 - self.dropout_rate,\n",
    "          (x.shape[0], 1, x.shape[-1]),\n",
    "      )\n",
    "      x = x * drop / (1 - self.dropout_rate)\n",
    "    return x\n",
    "\n",
    "\n",
    "def repeat_kv(x, n_rep):\n",
    "  bs, slen, n_kv_heads, head_dim = x.shape\n",
    "  if n_rep == 1:\n",
    "    return x\n",
    "  return jnp.tile(x[:, :, :, None, :], [1, 1, 1, n_rep, 1]).reshape(\n",
    "      bs, slen, n_kv_heads * n_rep, head_dim\n",
    "  )\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int | None = None\n",
    "    dropout_rate: float = 0.0\n",
    "    qkv_bias: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self._n_kv_heads = self.n_heads if self.n_kv_heads is None else self.n_kv_heads\n",
    "        assert self.n_heads % self._n_kv_heads == 0\n",
    "        self.n_rep = self.n_heads // self._n_kv_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.wq = nn.Dense(self.n_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wk = nn.Dense(self._n_kv_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wv = nn.Dense(self._n_kv_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wo = nn.Dense(self.dim, use_bias=False)\n",
    "        self.attn_dropout = nn.Dropout(self.dropout_rate)\n",
    "        # self.resid_dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.resid_dropout = Dropout1d(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, train=False):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.reshape(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(bsz, seqlen, self._n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(bsz, seqlen, self._n_kv_heads, self.head_dim)\n",
    "\n",
    "        # grouped multiquery attention: expand out keys and values\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # make heads into a batch dimension\n",
    "        xq = xq.swapaxes(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "        xk = xk.swapaxes(1, 2)\n",
    "        xv = xv.swapaxes(1, 2)\n",
    "\n",
    "        scores = jnp.matmul(xq, xk.swapaxes(2, 3)) / math.sqrt(self.head_dim)\n",
    "        scores = nn.softmax(scores, axis=-1)\n",
    "        scores = self.attn_dropout(scores, deterministic=not train)\n",
    "        output = jnp.matmul(scores, xv)  # (bs, n_heads, seqlen, head_dim)\n",
    "\n",
    "        # restore time as batch dimension and concat heads\n",
    "        output = output.swapaxes(1, 2).reshape(bsz, seqlen, -1)\n",
    "\n",
    "        # final projection into the residual stream\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output, deterministic=not train)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7194f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Attention Performance Test ===\n",
      "Config: batch_size=256, seq_len=128, dim=768\n",
      "Heads: n_heads=12, n_kv_heads=12\n",
      "\n",
      "Warming up JIT compilation...\n",
      "Warmup 1/5 complete\n",
      "Warmup 2/5 complete\n",
      "Warmup 3/5 complete\n",
      "Warmup 4/5 complete\n",
      "Warmup 5/5 complete\n",
      "Warmup complete!\n",
      "\n",
      "Measuring training performance...\n",
      "Measuring inference performance...\n",
      "\n",
      "=== Performance Results ===\n",
      "Training mode (50 runs):\n",
      "  Mean: 1.854ms ± 0.009ms\n",
      "  Range: 1.832ms - 1.890ms\n",
      "\n",
      "Inference mode (50 runs):\n",
      "  Mean: 0.972ms ± 0.037ms\n",
      "  Range: 0.942ms - 1.130ms\n",
      "\n",
      "Training overhead: 90.7%\n",
      "\n",
      "Model info:\n",
      "  Total parameters: 2,359,296\n",
      "  Tokens/sec (training): 17,672,133\n",
      "  Tokens/sec (inference): 33,700,979\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def performance_test():\n",
    "    # Test configuration\n",
    "    batch_size = 256\n",
    "    seq_len = 128\n",
    "    dim = 12 * 64\n",
    "    n_heads = 12\n",
    "    n_kv_heads = 12  # For grouped query attention\n",
    "    \n",
    "    # Create model and test data\n",
    "    model = Attention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.1)\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    params = model.init(key, x, train=True)\n",
    "    \n",
    "    # Create jitted function for training and inference\n",
    "    @jax.jit\n",
    "    def forward_train(params, x, key):\n",
    "        return model.apply(params, x, train=True, rngs={'dropout': key})\n",
    "    \n",
    "    @jax.jit\n",
    "    def forward_inference(params, x):\n",
    "        return model.apply(params, x, train=False)\n",
    "    \n",
    "    print(\"=== Attention Performance Test ===\")\n",
    "    print(f\"Config: batch_size={batch_size}, seq_len={seq_len}, dim={dim}\")\n",
    "    print(f\"Heads: n_heads={n_heads}, n_kv_heads={n_kv_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # Warmup runs\n",
    "    print(\"Warming up JIT compilation...\")\n",
    "    warmup_runs = 5\n",
    "    for i in range(warmup_runs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        _ = forward_train(params, x, subkey)\n",
    "        _ = forward_inference(params, x)\n",
    "        print(f\"Warmup {i+1}/{warmup_runs} complete\")\n",
    "    \n",
    "    # Ensure all computations are complete\n",
    "    jax.block_until_ready(forward_train(params, x, key))\n",
    "    jax.block_until_ready(forward_inference(params, x))\n",
    "    print(\"Warmup complete!\\n\")\n",
    "    \n",
    "    # Performance measurement\n",
    "    num_runs = 50\n",
    "    \n",
    "    # Training mode performance\n",
    "    print(\"Measuring training performance...\")\n",
    "    train_times = []\n",
    "    for i in range(num_runs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        start_time = time.perf_counter()\n",
    "        result = forward_train(params, x, subkey)\n",
    "        jax.block_until_ready(result)\n",
    "        end_time = time.perf_counter()\n",
    "        train_times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Inference mode performance\n",
    "    print(\"Measuring inference performance...\")\n",
    "    inference_times = []\n",
    "    for i in range(num_runs):\n",
    "        start_time = time.perf_counter()\n",
    "        result = forward_inference(params, x)\n",
    "        jax.block_until_ready(result)\n",
    "        end_time = time.perf_counter()\n",
    "        inference_times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "    \n",
    "    # Calculate statistics\n",
    "    train_mean = np.mean(train_times)\n",
    "    train_std = np.std(train_times)\n",
    "    train_min = np.min(train_times)\n",
    "    train_max = np.max(train_times)\n",
    "    \n",
    "    inference_mean = np.mean(inference_times)\n",
    "    inference_std = np.std(inference_times)\n",
    "    inference_min = np.min(inference_times)\n",
    "    inference_max = np.max(inference_times)\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n=== Performance Results ===\")\n",
    "    print(f\"Training mode ({num_runs} runs):\")\n",
    "    print(f\"  Mean: {train_mean:.3f}ms ± {train_std:.3f}ms\")\n",
    "    print(f\"  Range: {train_min:.3f}ms - {train_max:.3f}ms\")\n",
    "    print()\n",
    "    print(f\"Inference mode ({num_runs} runs):\")\n",
    "    print(f\"  Mean: {inference_mean:.3f}ms ± {inference_std:.3f}ms\")\n",
    "    print(f\"  Range: {inference_min:.3f}ms - {inference_max:.3f}ms\")\n",
    "    print()\n",
    "    print(f\"Training overhead: {((train_mean - inference_mean) / inference_mean * 100):.1f}%\")\n",
    "    \n",
    "    # Calculate throughput\n",
    "    total_params = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "    tokens_per_sec_train = (batch_size * seq_len) / (train_mean / 1000)\n",
    "    tokens_per_sec_inference = (batch_size * seq_len) / (inference_mean / 1000)\n",
    "    \n",
    "    print(f\"\\nModel info:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Tokens/sec (training): {tokens_per_sec_train:,.0f}\")\n",
    "    print(f\"  Tokens/sec (inference): {tokens_per_sec_inference:,.0f}\")\n",
    "\n",
    "# Run the performance test\n",
    "performance_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55d47662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "=== Attention Implementation Comparison ===\n",
      "Config: batch_size=256, seq_len=128, dim=768\n",
      "Heads: n_heads=12, n_kv_heads=12\n",
      "\n",
      "Warming up...\n",
      "Regular Attention: 1.027ms ± 1.825ms\n",
      "Flash Attention: 2.944ms ± 0.011ms\n",
      "Speedup: 0.35x\n"
     ]
    }
   ],
   "source": [
    "import jax.experimental.pallas.ops.tpu.flash_attention as flash_attention\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int | None = None\n",
    "    dropout_rate: float = 0.0\n",
    "    qkv_bias: bool = False\n",
    "    causal: bool = False  # Match original - no causal masking\n",
    "\n",
    "    def setup(self):\n",
    "        self._n_kv_heads = self.n_heads if self.n_kv_heads is None else self.n_kv_heads\n",
    "        assert self.n_heads % self._n_kv_heads == 0\n",
    "        self.n_rep = self.n_heads // self._n_kv_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        \n",
    "        # Use same initialization as original\n",
    "        self.wq = nn.Dense(self.n_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wk = nn.Dense(self._n_kv_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wv = nn.Dense(self._n_kv_heads * self.head_dim, use_bias=self.qkv_bias)\n",
    "        self.wo = nn.Dense(self.dim, use_bias=False)\n",
    "        \n",
    "        # Match original dropout setup exactly\n",
    "        self.attn_dropout = nn.Dropout(self.dropout_rate)\n",
    "        self.resid_dropout = Dropout1d(self.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, attention_bias=None, train=False):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # QKV projections - identical to original\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.reshape(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.reshape(bsz, seqlen, self._n_kv_heads, self.head_dim)\n",
    "        xv = xv.reshape(bsz, seqlen, self._n_kv_heads, self.head_dim)\n",
    "\n",
    "        # Grouped multiquery attention: expand out keys and values - identical to original\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # make heads into a batch dimension - identical to original\n",
    "        xq = xq.swapaxes(1, 2)  # (bs, n_heads, seqlen, head_dim)\n",
    "        xk = xk.swapaxes(1, 2)\n",
    "        xv = xv.swapaxes(1, 2)\n",
    "\n",
    "        # Scale factor - identical to original\n",
    "        sm_scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Create custom block sizes\n",
    "        block_sizes = flash_attention.BlockSizes(\n",
    "            block_b=64,\n",
    "            block_q=128,\n",
    "            block_k_major=128,\n",
    "            block_k=128\n",
    "        )\n",
    "        \n",
    "        output = flash_attention.flash_attention(\n",
    "            q=xq,\n",
    "            k=xk,\n",
    "            v=xv,\n",
    "            ab=attention_bias,\n",
    "            causal=self.causal,\n",
    "            sm_scale=sm_scale,\n",
    "            block_sizes=block_sizes,\n",
    "        )\n",
    "                \n",
    "        # restore time as batch dimension and concat heads - identical to original\n",
    "        output = output.swapaxes(1, 2).reshape(bsz, seqlen, -1)\n",
    "\n",
    "        # final projection into the residual stream - identical to original\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output, deterministic=not train)\n",
    "        return output\n",
    "\n",
    "\n",
    "def test_mathematical_equivalence():\n",
    "    \"\"\"Test that FlashAttention matches Attention exactly when using fallback.\"\"\"\n",
    "    print(\"=== Mathematical Equivalence Test ===\")\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size = 2\n",
    "    seq_len = 64\n",
    "    dim = 192\n",
    "    n_heads = 6\n",
    "    n_kv_heads = 6\n",
    "    \n",
    "    # Create models with same config\n",
    "    regular_attn = Attention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0, qkv_bias=False)\n",
    "    flash_attn = FlashAttention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0, qkv_bias=False, causal=False)\n",
    "    \n",
    "    # Test data\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    \n",
    "    # Initialize with same parameters by copying\n",
    "    regular_params = regular_attn.init(key, x, train=False)\n",
    "    flash_params = flash_attn.init(key, x, train=False)\n",
    "    \n",
    "    # Copy weights to ensure identical parameters\n",
    "    flash_params = {\n",
    "        'params': {\n",
    "            'wq': regular_params['params']['wq'],\n",
    "            'wk': regular_params['params']['wk'], \n",
    "            'wv': regular_params['params']['wv'],\n",
    "            'wo': regular_params['params']['wo'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Test inference (no dropout)\n",
    "    regular_out = regular_attn.apply(regular_params, x, train=False)\n",
    "    flash_out = flash_attn.apply(flash_params, x, train=False)\n",
    "    \n",
    "    # Check mathematical equivalence\n",
    "    max_diff = jnp.max(jnp.abs(regular_out - flash_out))\n",
    "    rel_diff = max_diff / jnp.max(jnp.abs(regular_out))\n",
    "    \n",
    "    print(f\"Output shapes: Regular {regular_out.shape}, Flash {flash_out.shape}\")\n",
    "    print(f\"Max absolute difference: {max_diff}\")\n",
    "    print(f\"Max relative difference: {rel_diff}\")\n",
    "    print(f\"Outputs are identical: {jnp.allclose(regular_out, flash_out, rtol=1e-6, atol=1e-6)}\")\n",
    "    \n",
    "    # Test with different sequence lengths and grouped query attention\n",
    "    print(\"\\nTesting with grouped query attention...\")\n",
    "    regular_attn_gqa = Attention(dim=dim, n_heads=n_heads, n_kv_heads=3, dropout_rate=0.0, qkv_bias=False)\n",
    "    flash_attn_gqa = FlashAttention(dim=dim, n_heads=n_heads, n_kv_heads=3, dropout_rate=0.0, qkv_bias=False, causal=False)\n",
    "    \n",
    "    regular_params_gqa = regular_attn_gqa.init(key, x, train=False)\n",
    "    flash_params_gqa = {\n",
    "        'params': {\n",
    "            'wq': regular_params_gqa['params']['wq'],\n",
    "            'wk': regular_params_gqa['params']['wk'], \n",
    "            'wv': regular_params_gqa['params']['wv'],\n",
    "            'wo': regular_params_gqa['params']['wo'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    regular_out_gqa = regular_attn_gqa.apply(regular_params_gqa, x, train=False)\n",
    "    flash_out_gqa = flash_attn_gqa.apply(flash_params_gqa, x, train=False)\n",
    "    \n",
    "    max_diff_gqa = jnp.max(jnp.abs(regular_out_gqa - flash_out_gqa))\n",
    "    print(f\"GQA Max absolute difference: {max_diff_gqa}\")\n",
    "    print(f\"GQA Outputs are identical: {jnp.allclose(regular_out_gqa, flash_out_gqa, rtol=1e-6, atol=1e-6)}\")\n",
    "\n",
    "\n",
    "def compare_attention_implementations():\n",
    "    \"\"\"Compare regular attention vs flash attention performance.\"\"\"\n",
    "    # Test configuration  \n",
    "    batch_size = 256\n",
    "    seq_len = 128\n",
    "    dim = 12 * 64\n",
    "    n_heads = 12\n",
    "    n_kv_heads = 12\n",
    "    \n",
    "    print(\"=== Attention Implementation Comparison ===\")\n",
    "    print(f\"Config: batch_size={batch_size}, seq_len={seq_len}, dim={dim}\")\n",
    "    print(f\"Heads: n_heads={n_heads}, n_kv_heads={n_kv_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # Create models with identical config\n",
    "    regular_attn = Attention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0)\n",
    "    flash_attn = FlashAttention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0, causal=False)\n",
    "    \n",
    "    # Test data\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    regular_params = regular_attn.init(key, x, train=False)\n",
    "    flash_params = flash_attn.init(key, x, train=False)\n",
    "    \n",
    "    # Create jitted functions\n",
    "    @jax.jit\n",
    "    def regular_forward(params, x):\n",
    "        return regular_attn.apply(params, x, train=False)\n",
    "    \n",
    "    @jax.jit \n",
    "    def flash_forward(params, x):\n",
    "        return flash_attn.apply(params, x, train=False)\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Warming up...\")\n",
    "    for _ in range(100):\n",
    "        _ = regular_forward(regular_params, x)\n",
    "        _ = flash_forward(flash_params, x)\n",
    "    \n",
    "    # Performance comparison\n",
    "    import time\n",
    "    num_runs = 1000\n",
    "    \n",
    "    # Regular attention timing\n",
    "    regular_times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = regular_forward(regular_params, x)\n",
    "        jax.block_until_ready(result)\n",
    "        regular_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    # Flash attention timing\n",
    "    flash_times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = flash_forward(flash_params, x)\n",
    "        jax.block_until_ready(result)\n",
    "        flash_times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    # Results\n",
    "    regular_mean = jnp.mean(jnp.array(regular_times))\n",
    "    flash_mean = jnp.mean(jnp.array(flash_times))\n",
    "    speedup = regular_mean / flash_mean\n",
    "    \n",
    "    print(f\"Regular Attention: {regular_mean:.3f}ms ± {jnp.std(jnp.array(regular_times)):.3f}ms\")\n",
    "    print(f\"Flash Attention: {flash_mean:.3f}ms ± {jnp.std(jnp.array(flash_times)):.3f}ms\")\n",
    "    print(f\"Speedup: {speedup:.2f}x\")\n",
    "\n",
    "# Test mathematical equivalence first\n",
    "# test_mathematical_equivalence()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Then compare performance\n",
    "try:\n",
    "    compare_attention_implementations()\n",
    "except Exception as e:\n",
    "    print(f\"Comparison failed: {e}\")\n",
    "    print(\"Flash attention may not be available on this platform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70104a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XLA Compilation Analysis ===\n",
      "Input config: batch_size=8, seq_len=127, dim=768\n",
      "Heads: n_heads=12, n_kv_heads=12\n",
      "\n",
      "Compiling regular attention...\n",
      "Compiling flash attention...\n",
      "Regular Attention HLO Analysis:\n",
      "========================================\n",
      "Padding operations found: 0\n",
      "\n",
      "Reshape operations found: 0\n",
      "\n",
      "Dot operations found: 13\n",
      "  1: ROOT %convolution-base-dilated.2 = bf16[8,12,127,64]{2,3,1,0:T(8,128)(2,1)S(1)} convolution(%fusion.1, %fusion.33), window={size=8x12 stride=7x11 lhs_dilate=8x12}, dim_labels=01bf_01io->01bf, metadata={op_name=\"jit(regular_forward)/jit(main)/Attention/dot_general\" source_file=\"/tmp/ipykernel_11883/206515483.py\" source_line=65}\n",
      "  2: %convolution-base-dilated.3 = f32[8,12,127,127]{2,3,1,0:T(8,128)S(1)} convolution(%fusion.31, %fusion.34), window={size=8x12 stride=7x11 lhs_dilate=8x12}, dim_labels=01bf_01io->01bf, metadata={op_name=\"jit(regular_forward)/jit(main)/Attention/dot_general\" source_file=\"/tmp/ipykernel_11883/206515483.py\" source_line=62}\n",
      "  3: ROOT %convolution.4 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.7, %fusion.35), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(regular_forward)/jit(main)/Attention/wv/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "  4: ROOT %convolution.5 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.9, %fusion.36), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(regular_forward)/jit(main)/Attention/wk/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "  5: ROOT %convolution.6 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.11, %fusion.37), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(regular_forward)/jit(main)/Attention/wq/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "\n",
      "========================================\n",
      "Flash Attention HLO Analysis:\n",
      "========================================\n",
      "Padding operations found: 0\n",
      "\n",
      "Reshape operations found: 0\n",
      "\n",
      "Dot operations found: 13\n",
      "  1: ROOT %convolution-base-dilated.2 = bf16[8,12,127,64]{2,3,1,0:T(8,128)(2,1)S(1)} convolution(%fusion.1, %fusion.33), window={size=8x12 stride=7x11 lhs_dilate=8x12}, dim_labels=01bf_01io->01bf, metadata={op_name=\"jit(flash_forward)/jit(main)/FlashAttention/dot_general\" source_file=\"/tmp/ipykernel_11883/3880927651.py\" source_line=76}\n",
      "  2: %convolution-base-dilated.3 = f32[8,12,127,127]{2,3,1,0:T(8,128)S(1)} convolution(%fusion.31, %fusion.34), window={size=8x12 stride=7x11 lhs_dilate=8x12}, dim_labels=01bf_01io->01bf, metadata={op_name=\"jit(flash_forward)/jit(main)/FlashAttention/dot_general\" source_file=\"/tmp/ipykernel_11883/3880927651.py\" source_line=73}\n",
      "  3: ROOT %convolution.4 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.7, %fusion.35), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(flash_forward)/jit(main)/FlashAttention/wv/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "  4: ROOT %convolution.5 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.9, %fusion.36), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(flash_forward)/jit(main)/FlashAttention/wk/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "  5: ROOT %convolution.6 = bf16[8,127,768]{1,2,0:T(8,128)(2,1)S(1)} convolution(%fusion.11, %fusion.37), window={size=1}, dim_labels=0bf_io0->0bf, metadata={op_name=\"jit(flash_forward)/jit(main)/FlashAttention/wq/dot_general\" source_file=\"/home/wuhao/md4/.venv/lib/python3.13/site-packages/flax/linen/linear.py\" source_line=288}\n",
      "\n",
      "Custom operations found: 0\n",
      "\n",
      "==================================================\n",
      "Memory Layout Analysis:\n",
      "==================================================\n",
      "\n",
      "Regular Intermediate Shapes:\n",
      "  Input: (8, 127, 768)\n",
      "Shape analysis failed: \"Attention\" object has no attribute \"wq\". If \"wq\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n",
      "\n",
      "==================================================\n",
      "Sequence Length Padding Test:\n",
      "==================================================\n",
      "  seq_len= 64: Regular=482.00ms, Flash=474.82ms, Speedup=1.02x\n",
      "  seq_len=127: Regular=0.20ms, Flash=0.17ms, Speedup=1.15x\n",
      "  seq_len=128: Regular=502.47ms, Flash=498.76ms, Speedup=1.01x\n",
      "  seq_len=129: Regular=1122.37ms, Flash=1088.95ms, Speedup=1.03x\n",
      "  seq_len=256: Regular=549.54ms, Flash=544.43ms, Speedup=1.01x\n",
      "  seq_len=511: Regular=1780.81ms, Flash=1745.14ms, Speedup=1.02x\n",
      "  seq_len=512: Regular=1053.89ms, Flash=905.68ms, Speedup=1.16x\n",
      "  seq_len=513: Regular=1981.04ms, Flash=1892.35ms, Speedup=1.05x\n"
     ]
    }
   ],
   "source": [
    "def analyze_xla_compilation():\n",
    "    \"\"\"Analyze XLA compilation and padding for attention implementations.\"\"\"\n",
    "    print(\"=== XLA Compilation Analysis ===\")\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size = 8\n",
    "    seq_len = 127  # Odd number to see padding effects\n",
    "    dim = 768\n",
    "    n_heads = 12\n",
    "    n_kv_heads = 12\n",
    "    \n",
    "    print(f\"Input config: batch_size={batch_size}, seq_len={seq_len}, dim={dim}\")\n",
    "    print(f\"Heads: n_heads={n_heads}, n_kv_heads={n_kv_heads}\")\n",
    "    print()\n",
    "    \n",
    "    # Create models\n",
    "    regular_attn = Attention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0)\n",
    "    flash_attn = FlashAttention(dim=dim, n_heads=n_heads, n_kv_heads=n_kv_heads, dropout_rate=0.0, causal=False)\n",
    "    \n",
    "    # Test data\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, dim))\n",
    "    \n",
    "    # Initialize parameters\n",
    "    regular_params = regular_attn.init(key, x, train=False)\n",
    "    flash_params = flash_attn.init(key, x, train=False)\n",
    "    \n",
    "    # Create functions for analysis\n",
    "    def regular_forward(params, x):\n",
    "        return regular_attn.apply(params, x, train=False)\n",
    "    \n",
    "    def flash_forward(params, x):\n",
    "        return flash_attn.apply(params, x, train=False)\n",
    "    \n",
    "    # Compile and get HLO\n",
    "    print(\"Compiling regular attention...\")\n",
    "    regular_compiled = jax.jit(regular_forward)\n",
    "    regular_out = regular_compiled(regular_params, x)  # Trigger compilation\n",
    "    \n",
    "    print(\"Compiling flash attention...\")\n",
    "    flash_compiled = jax.jit(flash_forward)\n",
    "    flash_out = flash_compiled(flash_params, x)  # Trigger compilation\n",
    "    \n",
    "    # Get HLO text representation\n",
    "    try:\n",
    "        # Get the compiled computation\n",
    "        regular_hlo = regular_compiled.lower(regular_params, x).compile().as_text()\n",
    "        flash_hlo = flash_compiled.lower(flash_params, x).compile().as_text()\n",
    "        \n",
    "        print(\"Regular Attention HLO Analysis:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Look for padding operations\n",
    "        regular_lines = regular_hlo.split('\\n')\n",
    "        padding_ops = [line for line in regular_lines if 'pad' in line.lower()]\n",
    "        reshape_ops = [line for line in regular_lines if 'reshape' in line.lower()]\n",
    "        dot_ops = [line for line in regular_lines if 'dot' in line.lower()]\n",
    "        \n",
    "        print(f\"Padding operations found: {len(padding_ops)}\")\n",
    "        for i, op in enumerate(padding_ops[:5]):  # Show first 5\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "        \n",
    "        print(f\"\\nReshape operations found: {len(reshape_ops)}\")\n",
    "        for i, op in enumerate(reshape_ops[:5]):  # Show first 5\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "            \n",
    "        print(f\"\\nDot operations found: {len(dot_ops)}\")\n",
    "        for i, op in enumerate(dot_ops[:5]):  # Show first 5\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 40)\n",
    "        print(\"Flash Attention HLO Analysis:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        flash_lines = flash_hlo.split('\\n')\n",
    "        flash_padding_ops = [line for line in flash_lines if 'pad' in line.lower()]\n",
    "        flash_reshape_ops = [line for line in flash_lines if 'reshape' in line.lower()]\n",
    "        flash_dot_ops = [line for line in flash_lines if 'dot' in line.lower()]\n",
    "        flash_custom_ops = [line for line in flash_lines if 'custom' in line.lower()]\n",
    "        \n",
    "        print(f\"Padding operations found: {len(flash_padding_ops)}\")\n",
    "        for i, op in enumerate(flash_padding_ops[:5]):\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "        \n",
    "        print(f\"\\nReshape operations found: {len(flash_reshape_ops)}\")\n",
    "        for i, op in enumerate(flash_reshape_ops[:5]):\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "            \n",
    "        print(f\"\\nDot operations found: {len(flash_dot_ops)}\")\n",
    "        for i, op in enumerate(flash_dot_ops[:5]):\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "            \n",
    "        print(f\"\\nCustom operations found: {len(flash_custom_ops)}\")\n",
    "        for i, op in enumerate(flash_custom_ops[:5]):\n",
    "            print(f\"  {i+1}: {op.strip()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract HLO: {e}\")\n",
    "    \n",
    "    # Memory usage analysis\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Memory Layout Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check tensor shapes at different stages\n",
    "    def analyze_intermediate_shapes(params, x, model_name):\n",
    "        print(f\"\\n{model_name} Intermediate Shapes:\")\n",
    "        \n",
    "        if model_name == \"Regular\":\n",
    "            model = regular_attn\n",
    "        else:\n",
    "            model = flash_attn\n",
    "            \n",
    "        # Manually trace through to see shapes\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        print(f\"  Input: {x.shape}\")\n",
    "        \n",
    "        # Get projected Q, K, V shapes\n",
    "        if model_name == \"Regular\":\n",
    "            with jax.disable_jit():\n",
    "                # Extract intermediate computations\n",
    "                xq = model.wq.apply({'params': params['params']['wq']}, x)\n",
    "                xk = model.wk.apply({'params': params['params']['wk']}, x)\n",
    "                xv = model.wv.apply({'params': params['params']['wv']}, x)\n",
    "                \n",
    "                print(f\"  Q projection: {xq.shape}\")\n",
    "                print(f\"  K projection: {xk.shape}\")\n",
    "                print(f\"  V projection: {xv.shape}\")\n",
    "                \n",
    "                # After reshape\n",
    "                xq_reshaped = xq.reshape(bsz, seqlen, n_heads, dim // n_heads)\n",
    "                xk_reshaped = xk.reshape(bsz, seqlen, n_kv_heads, dim // n_heads)\n",
    "                xv_reshaped = xv.reshape(bsz, seqlen, n_kv_heads, dim // n_heads)\n",
    "                \n",
    "                print(f\"  Q reshaped: {xq_reshaped.shape}\")\n",
    "                print(f\"  K reshaped: {xk_reshaped.shape}\")\n",
    "                print(f\"  V reshaped: {xv_reshaped.shape}\")\n",
    "                \n",
    "                # After transpose\n",
    "                xq_transposed = xq_reshaped.swapaxes(1, 2)\n",
    "                xk_transposed = xk_reshaped.swapaxes(1, 2)\n",
    "                xv_transposed = xv_reshaped.swapaxes(1, 2)\n",
    "                \n",
    "                print(f\"  Q transposed: {xq_transposed.shape}\")\n",
    "                print(f\"  K transposed: {xk_transposed.shape}\")\n",
    "                print(f\"  V transposed: {xv_transposed.shape}\")\n",
    "                \n",
    "                # Attention scores\n",
    "                scores = jnp.matmul(xq_transposed, xk_transposed.swapaxes(2, 3))\n",
    "                print(f\"  Attention scores: {scores.shape}\")\n",
    "                \n",
    "                # Memory usage estimation\n",
    "                total_memory = (\n",
    "                    jnp.prod(jnp.array(xq.shape)) + \n",
    "                    jnp.prod(jnp.array(xk.shape)) + \n",
    "                    jnp.prod(jnp.array(xv.shape)) + \n",
    "                    jnp.prod(jnp.array(scores.shape))\n",
    "                ) * 4  # Assuming float32\n",
    "                \n",
    "                print(f\"  Estimated memory (MB): {total_memory / (1024 * 1024):.2f}\")\n",
    "    \n",
    "    try:\n",
    "        analyze_intermediate_shapes(regular_params, x, \"Regular\")\n",
    "        analyze_intermediate_shapes(flash_params, x, \"Flash\")\n",
    "    except Exception as e:\n",
    "        print(f\"Shape analysis failed: {e}\")\n",
    "    \n",
    "    # Check for sequence length padding effects\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Sequence Length Padding Test:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test with different sequence lengths to see padding effects\n",
    "    test_seq_lens = [64, 127, 128, 129, 256, 511, 512, 513]\n",
    "    \n",
    "    for test_seq_len in test_seq_lens:\n",
    "        test_x = jax.random.normal(key, (batch_size, test_seq_len, dim))\n",
    "        \n",
    "        try:\n",
    "            # Time both implementations\n",
    "            start = time.perf_counter()\n",
    "            _ = regular_compiled(regular_params, test_x)\n",
    "            jax.block_until_ready(_)\n",
    "            regular_time = (time.perf_counter() - start) * 1000\n",
    "            \n",
    "            start = time.perf_counter()\n",
    "            _ = flash_compiled(flash_params, test_x)\n",
    "            jax.block_until_ready(_)\n",
    "            flash_time = (time.perf_counter() - start) * 1000\n",
    "            \n",
    "            speedup = regular_time / flash_time if flash_time > 0 else float('inf')\n",
    "            \n",
    "            print(f\"  seq_len={test_seq_len:3d}: Regular={regular_time:.2f}ms, Flash={flash_time:.2f}ms, Speedup={speedup:.2f}x\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  seq_len={test_seq_len:3d}: Error - {e}\")\n",
    "\n",
    "# Run XLA analysis\n",
    "import time\n",
    "analyze_xla_compilation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c641dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md4 (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
